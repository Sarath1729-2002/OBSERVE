{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12289458,"sourceType":"datasetVersion","datasetId":7745302},{"sourceId":12405831,"sourceType":"datasetVersion","datasetId":7823495}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom pathlib import Path\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport os\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\n\n\nfinal_cols = ['anger_intensity',\n'arousal_change_magnitude',\n'arousal_deviation_from_neutral',\n'arousal_onset_detected',\n'arousal_stability',\n'attention_focus_distracted',\n'attention_focus_focused',\n'attention_focus_moderate',\n'attention_stability_index',\n'avg_blink_duration_sec',\n'avg_fixation_duration_sec',\n'avg_saccade_amplitude',\n'behavioral_complexity',\n'behavioral_state_normal',\n'blink_completeness_score',\n'blink_rate_per_minute',\n'blink_rhythm_score',\n'cognitive_load_index',\n'disengagement_indicator',\n'disgust_intensity',\n'emotion_quadrant_negative_high_arousal',\n'emotion_quadrant_negative_low_arousal',\n'emotion_quadrant_neutral',\n'emotion_quadrant_positive_high_arousal',\n'emotion_transition_frequency',\n'engagement_proxy_score',\n'engagement_score',\n'engagement_state_low',\n'expression_arousal_sync',\n'expression_change_rate',\n'eye_openness_score',\n'eyebrow_furrow_intensity',\n'eyebrow_raise_intensity',\n'facial_asymmetry',\n'fixation_count_per_window',\n'frown_intensity',\n'gaze_consistency_score',\n'gaze_direction_center',\n'gaze_direction_down',\n'gaze_direction_down_left',\n'gaze_direction_down_right',\n'gaze_direction_left',\n'gaze_direction_right',\n'gaze_direction_up',\n'gaze_direction_up_left',\n'gaze_direction_up_right',\n'gaze_head_coordination',\n'head_Tx_velocity',\n'head_Ty_velocity',\n'head_Tz_velocity',\n'head_gaze_alignment_score',\n'head_movement_jerk',\n'head_movement_stability',\n'head_pitch',\n'head_pitch_acceleration',\n'head_pitch_velocity',\n'head_roll',\n'head_roll_acceleration',\n'head_roll_velocity',\n'head_tilt_direction_center',\n'head_tilt_direction_left',\n'head_yaw',\n'head_yaw_acceleration',\n'head_yaw_velocity',\n'left_eye_aperture',\n'micro_expression_frequency_per_min',\n'mouth_openness',\n'multimodal_consistency',\n'nostril_flare_intensity',\n'pupil_size_mean',\n'pupil_size_std',\n'right_eye_aperture',\n'saccade_frequency_per_sec',\n'sadness_intensity',\n'smile_intensity',\n'surprise_intensity',\n'temporal_alignment_score',\n'valence_deviation_from_neutral',\n'valence_stability']\n\n\ndef apply_one_hot_encoding(df: pd.DataFrame, columns_to_encode: list) -> pd.DataFrame:\n    \"\"\"\n    Applies one-hot encoding to specified categorical columns in a DataFrame.\n\n    Args:\n        df: The input pandas DataFrame containing the features.\n        columns_to_encode: A list of column names to be one-hot encoded.\n\n    Returns:\n        A new DataFrame with the specified columns one-hot encoded.\n        The original categorical columns will be dropped.\n    \"\"\"\n    \n    # Filter for columns that actually exist in the DataFrame\n    existing_columns_to_encode = [col for col in columns_to_encode if col in df.columns]\n    \n    if not existing_columns_to_encode:\n        print(\"No specified categorical columns found in the DataFrame to encode.\")\n        return df # Return original df if no columns exist\n        \n\n    # Use pd.get_dummies for one-hot encoding\n    # prefix ensures unique column names (e.g., 'head_tilt_direction_left')\n    # dtype=int ensures the new columns are integers (0 or 1)\n    df_encoded = pd.get_dummies(\n        df,\n        columns=existing_columns_to_encode,\n        prefix=existing_columns_to_encode,\n        dtype=int\n    )\n    \n    return df_encoded","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:51:36.655871Z","iopub.execute_input":"2025-07-08T07:51:36.656429Z","iopub.status.idle":"2025-07-08T07:51:36.665217Z","shell.execute_reply.started":"2025-07-08T07:51:36.656400Z","shell.execute_reply":"2025-07-08T07:51:36.664500Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Config:\n    # Data Configuration - 30 second windows for production\n    SEQUENCE_LENGTH = 750  # 30 seconds at 25 FPS\n    FEATURE_DIM = 456  # Your total features\n    FRAME_RATE = 25\n    \n    # Model Architecture - Optimized for 2x16GB\n    D_MODEL = 768\n    N_HEADS = 12\n    N_ENCODER_LAYERS = 8\n    DROPOUT = 0.1\n    \n    # SSL Configuration\n    SSL_TASKS = ['temporal_prediction', 'behavioral_consistency', 'attention_flow']\n    PREDICTION_HORIZON = 50  # Predict next 2 seconds\n    CONSISTENCY_WINDOW = 125  # 5 seconds for consistency check\n    \n    # Training - Full utilization of 2x16GB setup\n    BATCH_SIZE = 16  # Maximize GPU utilization\n    SSL_EPOCHS = 250  # Testing epochs\n    SSL_LR = 2e-4\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Data paths for Kaggle\n    TRAIN_PATH = '/kaggle/input/daisee-feature-processed/Train-final'\n    TEST_PATH = '/kaggle/input/daisee-feature-processed/Test-final'\n\nclass BehavioralDataset(Dataset):\n    \"\"\"Dataset for temporal behavioral data with SSL objectives\"\"\"\n    \n    def __init__(self, data_folder, sequence_length=750, is_train=True, max_files=None):\n        self.data_folder = Path(data_folder)\n        self.sequence_length = sequence_length\n        self.is_train = is_train\n        \n        # Collect all CSV files\n        self.csv_files = list(self.data_folder.glob(\"*.csv\"))\n        \n        # Limit files for testing if specified\n        if max_files:\n            self.csv_files = self.csv_files[:max_files]\n        \n        print(f\"Found {len(self.csv_files)} videos in {data_folder}\")\n        \n        # Get feature dimension from first file\n        if len(self.csv_files) > 0:\n            sample_df = pd.read_csv(self.csv_files[0])\n            self.feature_columns = final_cols\n            self.actual_feature_dim = len(final_cols)\n        else:\n            raise ValueError(\"No CSV files found in the directory\")\n    \n\n    \n    def __len__(self):\n        return len(self.csv_files)\n    \n    def __getitem__(self, idx):\n        try:\n            csv_path = self.csv_files[idx]\n            df = pd.read_csv(csv_path)\n            categorical_features_for_encoding = [\n                    \"head_tilt_direction\",\n                    \"emotion_quadrant\",\n                    \"engagement_state\",\n                    \"attention_focus\",\n                    \"behavioral_state\"\n                ]\n\n            df = apply_one_hot_encoding(df, categorical_features_for_encoding)\n            \n            if len(df) == 0:\n                print(f\"Warning: Empty dataframe in {csv_path}\")\n                # Return dummy data\n                features = np.zeros((self.sequence_length, self.actual_feature_dim), dtype=np.float32)\n            else:\n                # Use the pre-determined feature columns for consistency\n                for col in self.feature_columns:\n                    if col not in df.columns:\n                        df[col] = 0\n\n                feature_data = df[self.feature_columns].copy()\n                \n                # Convert all columns to numeric, forcing errors to NaN\n                for col in feature_data.columns:\n                    feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')\n\n                feature_data = feature_data.fillna(feature_data.mean()).fillna(0)\n                \n                # Handle variable length videos\n                if len(feature_data) < self.sequence_length:\n                    # Pad shorter videos by repeating last frame\n                    padding_needed = self.sequence_length - len(feature_data)\n                    if len(feature_data) > 0:\n                        last_values = feature_data.iloc[-1:].values\n                        padding = np.repeat(last_values, padding_needed, axis=0)\n                        features = np.vstack([feature_data.values, padding])\n                    else:\n                        features = np.zeros((self.sequence_length, len(feature_data.columns)), dtype=np.float32)\n                        \n                elif len(feature_data) > self.sequence_length:\n                    # For longer videos, sample random window during training\n                    if self.is_train:\n                        start_idx = np.random.randint(0, len(feature_data) - self.sequence_length + 1)\n                        features = feature_data.iloc[start_idx:start_idx + self.sequence_length].values\n                    else:\n                        # Use first window for validation\n                        features = feature_data.iloc[:self.sequence_length].values\n                else:\n                    features = feature_data.values\n                \n                # Ensure we have the right shape and type\n                features = features.astype(np.float32)\n                \n                # Handle any remaining NaN or inf values\n                features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)\n                \n                # Normalize features to prevent extreme values\n                features = np.clip(features, -10, 10)\n            \n            # Create SSL targets\n            ssl_targets = self._create_ssl_targets(features)\n            \n            return {\n                'features': torch.tensor(features),\n                'ssl_targets': ssl_targets,\n                'video_name': csv_path.stem\n            }\n            \n        except Exception as e:\n            print(f\"Error processing {csv_path}: {str(e)}\")\n            dummy_features = np.zeros((self.sequence_length, self.actual_feature_dim), dtype=np.float32)\n            ssl_targets = self._create_ssl_targets(dummy_features)\n            return {\n                'features': torch.tensor(dummy_features),\n                'ssl_targets': ssl_targets,\n                'video_name': f'error_{idx}'\n            }\n    \n    def _create_ssl_targets(self, features):\n        \"\"\"Create multiple SSL objectives with fixed tensor dimensions\"\"\"\n        targets = {}\n        seq_len, feat_dim = features.shape\n        \n        # 1. Temporal Prediction: Fixed to always produce same sequence length\n        horizon = Config.PREDICTION_HORIZON\n        \n        if seq_len > horizon:\n            # Take input context from beginning, predict future frames\n            context_frames = features[:-horizon]  # Remove last 'horizon' frames\n            future_frames = features[horizon:]    # Remove first 'horizon' frames\n            \n            # Ensure both have same length (should be seq_len - horizon)\n            min_len = min(len(context_frames), len(future_frames))\n            targets['temporal_context'] = torch.tensor(context_frames[:min_len], dtype=torch.float32)\n            targets['temporal_future'] = torch.tensor(future_frames[:min_len], dtype=torch.float32)\n        else:\n            # For short sequences, predict next frame\n            targets['temporal_context'] = torch.tensor(features[:-1] if seq_len > 1 else features, dtype=torch.float32)\n            targets['temporal_future'] = torch.tensor(features[1:] if seq_len > 1 else features, dtype=torch.float32)\n        \n        # 2. Behavioral Consistency: Use different feature subsets\n        # Split features into different behavioral modalities\n        third = feat_dim // 3\n        attention_features = features[:, :third] if third > 0 else features\n        engagement_features = features[:, third:2*third] if third > 0 else features\n        emotion_features = features[:, 2*third:] if third > 0 else features\n        \n        targets['attention_trajectory'] = torch.tensor(attention_features, dtype=torch.float32)\n        targets['engagement_trajectory'] = torch.tensor(engagement_features, dtype=torch.float32)\n        targets['emotion_trajectory'] = torch.tensor(emotion_features, dtype=torch.float32)\n        \n        # 3. Cross-modal alignment\n        targets['cross_modal_pairs'] = (\n            torch.tensor(attention_features, dtype=torch.float32),\n            torch.tensor(engagement_features, dtype=torch.float32)\n        )\n        \n        return targets\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):  # Increased for longer sequences\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]\n\nclass BehavioralTransformer(nn.Module):\n    \"\"\"Transformer designed for behavioral temporal sequences - Full scale\"\"\"\n    \n    def __init__(self, config, actual_feature_dim):\n        super().__init__()\n        self.config = config\n        self.actual_feature_dim = actual_feature_dim\n        \n        # Input projection with layer norm\n        self.input_projection = nn.Sequential(\n            nn.Linear(actual_feature_dim, config.D_MODEL),\n            nn.LayerNorm(config.D_MODEL),\n            nn.Dropout(config.DROPOUT)\n        )\n        \n        self.pos_encoding = PositionalEncoding(config.D_MODEL, max_len=config.SEQUENCE_LENGTH)\n        \n        # Transformer encoder - Full scale\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=config.D_MODEL,\n            nhead=config.N_HEADS,\n            dim_feedforward=config.D_MODEL * 4,\n            dropout=config.DROPOUT,\n            batch_first=True,\n            activation='gelu'\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, config.N_ENCODER_LAYERS)\n        \n        # SSL heads with better architectures\n        self.temporal_predictor = nn.Sequential(\n            nn.Linear(config.D_MODEL, config.D_MODEL),\n            nn.GELU(),\n            nn.LayerNorm(config.D_MODEL),\n            nn.Dropout(config.DROPOUT),\n            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n            nn.GELU(),\n            nn.Linear(config.D_MODEL // 2, actual_feature_dim)\n        )\n        \n        self.consistency_projector = nn.Sequential(\n            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n            nn.GELU(),\n            nn.LayerNorm(config.D_MODEL // 2),\n            nn.Dropout(config.DROPOUT),\n            nn.Linear(config.D_MODEL // 2, 256)  # Consistency embedding\n        )\n        \n        self.attention_flow_predictor = nn.Sequential(\n            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n            nn.GELU(),\n            nn.LayerNorm(config.D_MODEL // 2),\n            nn.Dropout(config.DROPOUT),\n            nn.Linear(config.D_MODEL // 2, max(actual_feature_dim // 3, 64))  # Attention flow\n        )\n        \n        # Cross-modal predictors\n        self.cross_modal_predictor = nn.Sequential(\n            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n            nn.GELU(),\n            nn.Dropout(config.DROPOUT),\n            nn.Linear(config.D_MODEL // 2, max(actual_feature_dim // 3, 64))\n        )\n    \n    def forward(self, x, return_embeddings=False):\n        # x shape: (batch, sequence, features)\n        batch_size, seq_len, _ = x.shape\n        \n        # Project to model dimension\n        x = self.input_projection(x)  # (batch, seq, d_model)\n        \n        # Add positional encoding\n        x = x.transpose(0, 1)  # (seq, batch, d_model)\n        x = self.pos_encoding(x)\n        x = x.transpose(0, 1)  # (batch, seq, d_model)\n        \n        # Transformer encoding\n        embeddings = self.transformer(x)  # (batch, seq, d_model)\n        \n        if return_embeddings:\n            return embeddings\n        \n        # SSL predictions\n        ssl_outputs = {}\n        \n        # Temporal prediction - predict for reduced sequence length\n        prediction_length = seq_len - Config.PREDICTION_HORIZON\n        if prediction_length > 0:\n            temporal_embeddings = embeddings[:, :prediction_length]  # Match context length\n            ssl_outputs['temporal'] = self.temporal_predictor(temporal_embeddings)\n        else:\n            # Fallback for short sequences\n            ssl_outputs['temporal'] = self.temporal_predictor(embeddings[:, :-1] if seq_len > 1 else embeddings)\n        \n        # Behavioral consistency (use mean pooling)\n        ssl_outputs['consistency'] = self.consistency_projector(embeddings.mean(dim=1))\n        \n        # Attention flow\n        ssl_outputs['attention_flow'] = self.attention_flow_predictor(embeddings)\n        \n        # Cross-modal prediction\n        ssl_outputs['cross_modal'] = self.cross_modal_predictor(embeddings)\n        \n        return ssl_outputs\n\nclass BehavioralSSLLoss(nn.Module):\n    \"\"\"Multi-objective SSL loss for behavioral data with fixed tensor handling\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.mse_loss = nn.MSELoss()\n        self.cosine_loss = nn.CosineEmbeddingLoss()\n        self.huber_loss = nn.HuberLoss(delta=1.0)\n        \n    def forward(self, predictions, targets):\n        total_loss = 0\n        loss_components = {}\n        batch_size = None\n        device = None\n        \n        # Get batch size and device from any available tensor\n        for key, value in predictions.items():\n            if isinstance(value, torch.Tensor):\n                batch_size = value.size(0)\n                device = value.device\n                break\n        \n        if batch_size is None or device is None:\n            # Create a default tensor on CPU if no predictions available\n            device = torch.device('cpu')\n            return torch.tensor(0.0, device=device), {}\n        \n        # 1. Temporal Prediction Loss (main objective)\n        if 'temporal_future' in targets and 'temporal' in predictions:\n            future_frames = targets['temporal_future']\n            pred_frames = predictions['temporal']\n            \n            # Ensure both tensors are on the same device\n            if future_frames.device != device:\n                future_frames = future_frames.to(device)\n            \n            # Ensure batch dimension matches\n            if future_frames.size(0) == batch_size and pred_frames.size(0) == batch_size:\n                # Match sequence lengths\n                min_seq_len = min(future_frames.size(1), pred_frames.size(1))\n                min_feat_dim = min(future_frames.size(2), pred_frames.size(2))\n                \n                if min_seq_len > 0 and min_feat_dim > 0:\n                    temporal_loss = self.huber_loss(\n                        pred_frames[:, :min_seq_len, :min_feat_dim], \n                        future_frames[:, :min_seq_len, :min_feat_dim]\n                    )\n                    loss_components['temporal'] = temporal_loss\n                    total_loss += 2.0 * temporal_loss  # Higher weight for main task\n        \n        # 2. Behavioral Consistency Loss\n        if 'attention_trajectory' in targets and 'consistency' in predictions:\n            try:\n                consistency_loss = self._compute_consistency_loss(\n                    predictions['consistency'], \n                    targets['attention_trajectory'],\n                    device\n                )\n                loss_components['consistency'] = consistency_loss\n                total_loss += 0.5 * consistency_loss\n            except Exception as e:\n                print(f\"Consistency loss error: {e}\")\n        \n        # 3. Attention Flow Smoothness Loss\n        if 'attention_flow' in predictions:\n            try:\n                flow_loss = self._compute_flow_smoothness_loss(\n                    predictions['attention_flow']\n                )\n                loss_components['flow'] = flow_loss\n                total_loss += 0.3 * flow_loss\n            except Exception as e:\n                print(f\"Flow loss error: {e}\")\n        \n        # 4. Cross-modal alignment loss\n        if 'cross_modal_pairs' in targets and 'cross_modal' in predictions:\n            try:\n                cross_modal_loss = self._compute_cross_modal_loss(\n                    predictions['cross_modal'],\n                    targets['cross_modal_pairs'],\n                    device\n                )\n                loss_components['cross_modal'] = cross_modal_loss\n                total_loss += 0.4 * cross_modal_loss\n            except Exception as e:\n                print(f\"Cross-modal loss error: {e}\")\n        \n        # Ensure we have at least some loss\n        if total_loss == 0:\n            total_loss = torch.tensor(0.001, device=device, requires_grad=True)\n        \n        return total_loss, loss_components\n    \n    def _compute_consistency_loss(self, embeddings, attention_trajectories, device):\n        \"\"\"Encourage similar attention patterns to have similar embeddings\"\"\"\n        batch_size = embeddings.size(0)\n        \n        if batch_size < 2:\n            return torch.tensor(0.0, device=device)\n        \n        # Ensure attention_trajectories is on the correct device\n        if attention_trajectories.device != device:\n            attention_trajectories = attention_trajectories.to(device)\n        \n        # Compute attention similarity matrix\n        attention_flat = attention_trajectories.view(batch_size, -1)\n        attention_sim = F.cosine_similarity(\n            attention_flat.unsqueeze(1), \n            attention_flat.unsqueeze(0), \n            dim=2\n        )\n        \n        # Compute embedding similarity matrix\n        embedding_sim = F.cosine_similarity(\n            embeddings.unsqueeze(1), \n            embeddings.unsqueeze(0), \n            dim=2\n        )\n        \n        # Loss: embedding similarity should match attention similarity\n        consistency_loss = self.mse_loss(embedding_sim, attention_sim)\n        return consistency_loss\n    \n    def _compute_flow_smoothness_loss(self, attention_flow):\n        \"\"\"Encourage smooth attention transitions\"\"\"\n        if attention_flow.size(1) <= 1:\n            return torch.tensor(0.0, device=attention_flow.device)\n            \n        # Compute temporal differences\n        flow_diff = attention_flow[:, 1:] - attention_flow[:, :-1]\n        \n        # Penalize large jumps in attention flow\n        smoothness_loss = torch.mean(torch.abs(flow_diff))\n        return smoothness_loss\n    \n    def _compute_cross_modal_loss(self, predictions, target_pairs, device):\n        \"\"\"Cross-modal alignment loss\"\"\"\n        attention_targets, engagement_targets = target_pairs\n        \n        # Ensure all tensors are on the correct device\n        if attention_targets.device != device:\n            attention_targets = attention_targets.to(device)\n        if engagement_targets.device != device:\n            engagement_targets = engagement_targets.to(device)\n        \n        # Predict engagement from attention features\n        pred_engagement = predictions\n        target_engagement = engagement_targets.mean(dim=1)  # Pool over sequence: [batch, features]\n        \n        # Pool predictions over sequence to match target dimensions\n        pred_engagement_pooled = pred_engagement.mean(dim=1)  # [batch, seq, features] -> [batch, features]\n        \n        # Match dimensions\n        min_dim = min(pred_engagement_pooled.size(-1), target_engagement.size(-1))\n        \n        if min_dim > 0:\n            cross_modal_loss = self.mse_loss(\n                pred_engagement_pooled[:, :min_dim], \n                target_engagement[:, :min_dim]\n            )\n            return cross_modal_loss\n        else:\n            # Return zero loss tensor on the correct device\n            return torch.tensor(0.0, device=device, requires_grad=True)\n\nclass BehavioralSSLTrainer:\n    \"\"\"SSL Trainer for behavioral data - Full GPU utilization\"\"\"\n    \n    def __init__(self, config, actual_feature_dim):\n        self.config = config\n        self.device = config.DEVICE\n        \n        # Model\n        self.model = BehavioralTransformer(config, actual_feature_dim).to(self.device)\n        \n        # Data parallel if multiple GPUs - maximize utilization\n        if torch.cuda.device_count() > 1:\n            print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n            self.model = nn.DataParallel(self.model)\n        \n        # Loss\n        self.criterion = BehavioralSSLLoss(config)\n        \n        # Optimizer - higher learning rate for larger model\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(), \n            lr=config.SSL_LR,\n            weight_decay=1e-4,\n            betas=(0.9, 0.95),  # Better for transformers\n            eps=1e-8\n        )\n        \n        # Scheduler with warmup\n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            self.optimizer,\n            max_lr=config.SSL_LR,\n            epochs=config.SSL_EPOCHS,\n            steps_per_epoch=100,  # Approximate\n            pct_start=0.1,\n            anneal_strategy='cos'\n        )\n        \n        # Gradient scaler for mixed precision\n        self.scaler = torch.cuda.amp.GradScaler() if self.device == 'cuda' else None\n    \n    def train_epoch(self, dataloader):\n        self.model.train()\n        total_loss = 0\n        loss_components_sum = {}\n        num_batches = 0\n        \n        pbar = tqdm(dataloader, desc='Training')\n        for batch_idx, batch in enumerate(pbar):\n            try:\n                features = batch['features'].to(self.device, non_blocking=True)\n                ssl_targets = batch['ssl_targets']\n                \n                # Move targets to device - Fixed to handle all tensor types properly\n                for key, value in ssl_targets.items():\n                    if isinstance(value, torch.Tensor):\n                        ssl_targets[key] = value.to(self.device, non_blocking=True)\n                    elif isinstance(value, tuple):\n                        ssl_targets[key] = tuple(\n                            v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n                            for v in value\n                        )\n                \n                # Forward pass with mixed precision\n                if self.scaler is not None:\n                    with torch.cuda.amp.autocast():\n                        predictions = self.model(features)\n                        loss, loss_components = self.criterion(predictions, ssl_targets)\n                    \n                    # Backward pass\n                    self.optimizer.zero_grad()\n                    self.scaler.scale(loss).backward()\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                else:\n                    predictions = self.model(features)\n                    loss, loss_components = self.criterion(predictions, ssl_targets)\n                    \n                    # Backward pass\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                    self.optimizer.step()\n                \n                self.scheduler.step()\n                \n                total_loss += loss.item()\n                num_batches += 1\n                \n                # Accumulate loss components\n                for key, value in loss_components.items():\n                    if key not in loss_components_sum:\n                        loss_components_sum[key] = 0\n                    loss_components_sum[key] += value.item()\n                \n                pbar.set_postfix({\n                    'loss': f'{loss.item():.4f}',\n                    'lr': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}',\n                    'gpu_mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'\n                })\n                \n            except Exception as e:\n                print(f\"Error in batch {batch_idx}: {str(e)}\")\n                continue\n        \n        return total_loss / max(num_batches, 1), loss_components_sum\n    \n    def validate(self, dataloader):\n        self.model.eval()\n        total_loss = 0\n        num_batches = 0\n        \n        with torch.no_grad():\n            pbar = tqdm(dataloader, desc='Validation')\n            for batch in pbar:\n                try:\n                    features = batch['features'].to(self.device, non_blocking=True)\n                    ssl_targets = batch['ssl_targets']\n                    \n                    # Move targets to device - Fixed to handle all tensor types properly\n                    for key, value in ssl_targets.items():\n                        if isinstance(value, torch.Tensor):\n                            ssl_targets[key] = value.to(self.device, non_blocking=True)\n                        elif isinstance(value, tuple):\n                            ssl_targets[key] = tuple(\n                                v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n                                for v in value\n                            )\n                    \n                    if self.scaler is not None:\n                        with torch.cuda.amp.autocast():\n                            predictions = self.model(features)\n                            loss, _ = self.criterion(predictions, ssl_targets)\n                    else:\n                        predictions = self.model(features)\n                        loss, _ = self.criterion(predictions, ssl_targets)\n                    \n                    total_loss += loss.item()\n                    num_batches += 1\n                    \n                    pbar.set_postfix({\n                        'val_loss': f'{loss.item():.4f}',\n                        'gpu_mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'\n                    })\n                    \n                except Exception as e:\n                    print(f\"Error in validation batch: {str(e)}\")\n                    continue\n        \n        return total_loss / max(num_batches, 1)\n    \n    def save_model(self, path):\n        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n        torch.save({\n            'model_state_dict': model_to_save.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'config': self.config,\n            'actual_feature_dim': model_to_save.actual_feature_dim\n        }, path)\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path, map_location=self.device)\n        if hasattr(self.model, 'module'):\n            self.model.module.load_state_dict(checkpoint['model_state_dict'])\n        else:\n            self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if 'scheduler_state_dict' in checkpoint:\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\n\n\ndef test_dataset_loading(data_path, max_files=5):\n    \"\"\"Test dataset loading with detailed diagnostics\"\"\"\n    print(f\"Testing dataset loading from: {data_path}\")\n    \n    if not os.path.exists(data_path):\n        print(f\"Path does not exist: {data_path}\")\n        return False\n    \n    try:\n        test_dataset = BehavioralDataset(data_path, sequence_length=750, is_train=True, max_files=max_files)\n        \n        if len(test_dataset) == 0:\n            print(\"No data found in dataset\")\n            return False\n        \n        # Test loading multiple samples\n        print(f\"\\nTesting {min(3, len(test_dataset))} samples:\")\n        for i in range(min(3, len(test_dataset))):\n            sample = test_dataset[i]\n            print(f\"Sample {i}:\")\n            print(f\"  Features shape: {sample['features'].shape}\")\n            print(f\"  Video name: {sample['video_name']}\")\n            print(f\"  SSL targets keys: {list(sample['ssl_targets'].keys())}\")\n            \n            # Check temporal target shapes\n            if 'temporal_future' in sample['ssl_targets']:\n                print(f\"  Temporal future shape: {sample['ssl_targets']['temporal_future'].shape}\")\n            if 'temporal_context' in sample['ssl_targets']:\n                print(f\"  Temporal context shape: {sample['ssl_targets']['temporal_context'].shape}\")\n            \n            # Check for actual data (not all zeros)\n            if torch.sum(torch.abs(sample['features'])) > 0:\n                print(f\"  ✓ Contains non-zero data\")\n            else:\n                print(f\"  ⚠ Warning: All zeros detected\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Error testing dataset: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:51:39.375837Z","iopub.execute_input":"2025-07-08T07:51:39.376129Z","iopub.status.idle":"2025-07-08T07:51:39.431385Z","shell.execute_reply.started":"2025-07-08T07:51:39.376110Z","shell.execute_reply":"2025-07-08T07:51:39.430670Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n\n\n\ndef main():\n    config = Config()\n    \n    print(\"=== Behavioral SSL Training - Full Scale ===\")\n    print(f\"Device: {config.DEVICE}\")\n    print(f\"Available GPUs: {torch.cuda.device_count()}\")     \n    print(f\"Sequence Length: {config.SEQUENCE_LENGTH} frames (30 seconds)\")\n    print(f\"Batch Size: {config.BATCH_SIZE}\")\n    print(f\"Model Size: D_MODEL={config.D_MODEL}, Layers={config.N_ENCODER_LAYERS}\")\n    \n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n            print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB\")\n    \n    # Test dataset loading first\n    print(\"\\n=== Testing Dataset Loading ===\")\n    if not test_dataset_loading(config.TRAIN_PATH, max_files=3):\n        print(\"Dataset loading test failed. Please check your data paths and format.\")\n        return\n    \n    print(\"\\n=== Creating Datasets ===\")\n    try:\n \n        train_dataset = BehavioralDataset(\n            config.TRAIN_PATH, \n            sequence_length=config.SEQUENCE_LENGTH, \n            is_train=True\n        )\n        \n        val_dataset = BehavioralDataset(\n            config.TEST_PATH, \n            sequence_length=config.SEQUENCE_LENGTH, \n            is_train=False\n        )\n        \n        print(f\"Train samples: {len(train_dataset)}\")\n        print(f\"Validation samples: {len(val_dataset)}\")\n        \n        # Get actual feature dimension\n        sample = train_dataset[0]\n        actual_feature_dim = sample['features'].shape[-1]\n        print(f\"Actual feature dimension: {actual_feature_dim}\")\n        \n    except Exception as e:\n        print(f\"Error creating datasets: {str(e)}\")\n        return\n    \n    # Create data loaders with optimized settings\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config.BATCH_SIZE,\n        shuffle=True,\n        num_workers=2,  # Reduced for stability\n        pin_memory=True if config.DEVICE == 'cuda' else False,\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True if config.DEVICE == 'cuda' else False,\n        drop_last=False\n    )\n    \n    print(f\"Train batches: {len(train_loader)}\")\n    print(f\"Validation batches: {len(val_loader)}\")\n    \n    # Initialize trainer\n    print(\"\\n=== Initializing Model ===\")\n    trainer = BehavioralSSLTrainer(config, actual_feature_dim)\n    \n    # Model info\n    total_params = sum(p.numel() for p in trainer.model.parameters())\n    trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    print(f\"Model size: ~{total_params * 4 / 1e9:.2f}GB (FP32)\")\n    \n    # Training loop\n    print(\"\\n=== Starting SSL Training ===\")\n    \n    # --- EARLY STOPPING PARAMETERS ---\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    early_stopping_patience = 5  # Number of epochs to wait for improvement\n    min_delta = 0.005            # Minimum change in validation loss to count as an improvement\n    # --- END EARLY STOPPING PARAMETERS ---\n\n    for epoch in range(config.SSL_EPOCHS):\n        print(f\"\\nEpoch {epoch + 1}/{config.SSL_EPOCHS}\")\n        print(\"-\" * 50)\n        \n        # Training\n        train_loss, train_components = trainer.train_epoch(train_loader)\n        \n        # Validation\n        val_loss = trainer.validate(val_loader)\n        \n        # Print epoch results\n        print(f\"\\nEpoch {epoch + 1} Results:\")\n        print(f\"Train Loss: {train_loss:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}\")\n        \n        if train_components:\n            print(\"Training Loss Components:\")\n            for component, value in train_components.items():\n                avg_value = value / len(train_loader)\n                print(f\"  {component}: {avg_value:.4f}\")\n        \n        # --- EARLY STOPPING LOGIC ---\n        # Check if validation loss has improved by at least min_delta\n        if val_loss < best_val_loss - min_delta:\n            best_val_loss = val_loss\n            epochs_no_improve = 0 # Reset patience\n            print(f\"New best validation loss: {val_loss:.4f}. Saving model...\")\n            trainer.save_model('best_behavioral_ssl_model.pth') # Save the best model\n            print(\"Model saved!\")\n        else:\n            epochs_no_improve += 1\n            print(f\"Validation loss did not improve by {min_delta:.4f}. Patience: {epochs_no_improve}/{early_stopping_patience}\")\n\n        if epochs_no_improve >= early_stopping_patience:\n            print(f\"\\nEarly stopping triggered after {early_stopping_patience} epochs without significant improvement.\")\n            break # Exit the training loop\n        # --- END EARLY STOPPING LOGIC ---\n        \n        # Memory cleanup\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB allocated, \"\n                  f\"{torch.cuda.memory_reserved()/1e9:.1f}GB reserved\")\n    \n    print(\"\\n=== Training Complete ===\")\n    print(f\"Final best validation loss: {best_val_loss:.4f}\")\n    print(\"Best model checkpoint saved as 'best_behavioral_ssl_model.pth'\")\n    \n    # Test model inference\n    print(\"\\n=== Testing Model Inference ===\")\n    try:\n        # Load the best model if it was saved, before inference\n        print(\"Loading best model for inference test...\")\n        trainer.model.load_state_dict(torch.load('best_behavioral_ssl_model.pth'))\n        trainer.model.eval()\n        \n        test_batch = next(iter(val_loader))\n        test_features = test_batch['features'].to(config.DEVICE)\n        \n        with torch.no_grad():\n            # Test embeddings extraction\n            embeddings = trainer.model(test_features, return_embeddings=True)\n            print(f\"Embeddings shape: {embeddings.shape}\")\n            \n            # Test SSL predictions\n            predictions = trainer.model(test_features)\n            print(\"SSL prediction shapes:\")\n            for key, value in predictions.items():\n                print(f\"  {key}: {value.shape}\")\n        \n        print(\"✓ Model inference test successful!\")\n        \n    except Exception as e:\n        print(f\"Model inference test failed: {str(e)}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:51:42.848990Z","iopub.execute_input":"2025-07-08T07:51:42.849346Z","iopub.status.idle":"2025-07-08T07:52:01.157837Z","shell.execute_reply.started":"2025-07-08T07:51:42.849322Z","shell.execute_reply":"2025-07-08T07:52:01.156586Z"}},"outputs":[{"name":"stdout","text":"=== Behavioral SSL Training - Full Scale ===\nDevice: cuda\nAvailable GPUs: 2\nSequence Length: 750 frames (30 seconds)\nBatch Size: 16\nModel Size: D_MODEL=768, Layers=8\nGPU 0: Tesla T4\n  Memory: 15.8GB\nGPU 1: Tesla T4\n  Memory: 15.8GB\n\n=== Testing Dataset Loading ===\nTesting dataset loading from: /kaggle/input/daisee-feature-processed/Train-final\nFound 3 videos in /kaggle/input/daisee-feature-processed/Train-final\n\nTesting 3 samples:\nSample 0:\n  Features shape: torch.Size([750, 79])\n  Video name: 4100241051\n  SSL targets keys: ['temporal_context', 'temporal_future', 'attention_trajectory', 'engagement_trajectory', 'emotion_trajectory', 'cross_modal_pairs']\n  Temporal future shape: torch.Size([700, 79])\n  Temporal context shape: torch.Size([700, 79])\n  ✓ Contains non-zero data\nSample 1:\n  Features shape: torch.Size([750, 79])\n  Video name: 1100072077\n  SSL targets keys: ['temporal_context', 'temporal_future', 'attention_trajectory', 'engagement_trajectory', 'emotion_trajectory', 'cross_modal_pairs']\n  Temporal future shape: torch.Size([700, 79])\n  Temporal context shape: torch.Size([700, 79])\n  ✓ Contains non-zero data\nSample 2:\n  Features shape: torch.Size([750, 79])\n  Video name: 522129024\n  SSL targets keys: ['temporal_context', 'temporal_future', 'attention_trajectory', 'engagement_trajectory', 'emotion_trajectory', 'cross_modal_pairs']\n  Temporal future shape: torch.Size([700, 79])\n  Temporal context shape: torch.Size([700, 79])\n  ✓ Contains non-zero data\n\n=== Creating Datasets ===\nFound 6511 videos in /kaggle/input/daisee-feature-processed/Train-final\nFound 1720 videos in /kaggle/input/daisee-feature-processed/Test-final\nTrain samples: 6511\nValidation samples: 1720\nActual feature dimension: 79\nTrain batches: 406\nValidation batches: 108\n\n=== Initializing Model ===\nUsing 2 GPUs with DataParallel\nTotal parameters: 58,719,055\nTrainable parameters: 58,719,055\nModel size: ~0.23GB (FP32)\n\n=== Starting SSL Training ===\n\nEpoch 1/250\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   8%|▊         | 32/406 [00:17<03:25,  1.82it/s, loss=5.1867, lr=8.08e-06, gpu_mem=3.9GB]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2700331179.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2700331179.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/183210948.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     ) -> List[T]:\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     def scatter(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mparam_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mparam_copies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_coalesced_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mbuffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Use the autograd function to broadcast if not detach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mtensor_copies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             return [\n\u001b[1;32m    105\u001b[0m                 \u001b[0mtensor_copies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}