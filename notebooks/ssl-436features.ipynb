{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-28T00:07:40.612811Z",
     "iopub.status.busy": "2025-06-28T00:07:40.612064Z",
     "iopub.status.idle": "2025-06-28T00:07:40.676550Z",
     "shell.execute_reply": "2025-06-28T00:07:40.675764Z",
     "shell.execute_reply.started": "2025-06-28T00:07:40.612778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    # Data Configuration - 30 second windows for production\n",
    "    SEQUENCE_LENGTH = 750  # 30 seconds at 25 FPS\n",
    "    FEATURE_DIM = 456  # Your total features\n",
    "    FRAME_RATE = 25\n",
    "    \n",
    "    # Model Architecture - Optimized for 2x16GB\n",
    "    D_MODEL = 768\n",
    "    N_HEADS = 12\n",
    "    N_ENCODER_LAYERS = 8\n",
    "    DROPOUT = 0.1\n",
    "    \n",
    "    # SSL Configuration\n",
    "    SSL_TASKS = ['temporal_prediction', 'behavioral_consistency', 'attention_flow']\n",
    "    PREDICTION_HORIZON = 50  # Predict next 2 seconds\n",
    "    CONSISTENCY_WINDOW = 125  # 5 seconds for consistency check\n",
    "    \n",
    "    # Training - Full utilization of 2x16GB setup\n",
    "    BATCH_SIZE = 16  # Maximize GPU utilization\n",
    "    SSL_EPOCHS = 250  # Testing epochs\n",
    "    SSL_LR = 2e-4\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Data paths for Kaggle\n",
    "    TRAIN_PATH = '/kaggle/input/daisee-feature-extracted/Train-extracted'\n",
    "    TEST_PATH = '/kaggle/input/daisee-feature-extracted/Test-extracted'\n",
    "\n",
    "class BehavioralDataset(Dataset):\n",
    "    \"\"\"Dataset for temporal behavioral data with SSL objectives\"\"\"\n",
    "    \n",
    "    def __init__(self, data_folder, sequence_length=750, is_train=True, max_files=None):\n",
    "        self.data_folder = Path(data_folder)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Collect all CSV files\n",
    "        self.csv_files = list(self.data_folder.glob(\"*.csv\"))\n",
    "        \n",
    "        # Limit files for testing if specified\n",
    "        if max_files:\n",
    "            self.csv_files = self.csv_files[:max_files]\n",
    "        \n",
    "        print(f\"Found {len(self.csv_files)} videos in {data_folder}\")\n",
    "        \n",
    "        # Get feature dimension from first file\n",
    "        if len(self.csv_files) > 0:\n",
    "            sample_df = pd.read_csv(self.csv_files[0])\n",
    "            feature_cols = self._get_feature_columns(sample_df)\n",
    "            self.actual_feature_dim = len(feature_cols)\n",
    "\n",
    "            \n",
    "            # Store feature columns for consistent ordering\n",
    "            self.feature_columns = feature_cols\n",
    "        else:\n",
    "            raise ValueError(\"No CSV files found in the directory\")\n",
    "    \n",
    "    def _get_feature_columns(self, df):\n",
    "        \"\"\"Get feature columns, being more inclusive with column selection\"\"\"\n",
    "        # Basic metadata columns to exclude\n",
    "        exclude_cols = {'timestamp', 'frame_idx', 'video_name', 'video_path', 'face_id'}\n",
    "        \n",
    "        # Get all columns except the excluded ones\n",
    "        potential_cols = [col for col in df.columns if col.lower() not in exclude_cols]\n",
    "        \n",
    "        # More inclusive numeric column detection\n",
    "        numeric_cols = []\n",
    "        for col in potential_cols:\n",
    "            try:\n",
    "                # Try to convert the entire column to numeric\n",
    "                test_series = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                # Keep column if most values are numeric (allow some NaN)\n",
    "                non_nan_ratio = test_series.notna().mean()\n",
    "                if non_nan_ratio > 0.5:  # Keep if at least 50% of values are numeric\n",
    "                    numeric_cols.append(col)\n",
    "                else:\n",
    "                    print(f\"Excluding column '{col}': only {non_nan_ratio:.2%} numeric values\")\n",
    "            except Exception as e:\n",
    "                print(f\"Excluding column '{col}': {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Selected {len(numeric_cols)} numeric columns out of {len(potential_cols)} potential columns\")\n",
    "        return numeric_cols\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            csv_path = self.csv_files[idx]\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(f\"Warning: Empty dataframe in {csv_path}\")\n",
    "                # Return dummy data\n",
    "                features = np.zeros((self.sequence_length, self.actual_feature_dim), dtype=np.float32)\n",
    "            else:\n",
    "                # Use the pre-determined feature columns for consistency\n",
    "                feature_data = df[self.feature_columns].copy()\n",
    "                \n",
    "                # Convert all columns to numeric, forcing errors to NaN\n",
    "                for col in self.feature_columns:\n",
    "                    feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')\n",
    "                \n",
    "                # Fill NaN values with column means or 0 if all NaN\n",
    "                feature_data = feature_data.fillna(feature_data.mean()).fillna(0)\n",
    "                \n",
    "                # Handle variable length videos\n",
    "                if len(feature_data) < self.sequence_length:\n",
    "                    # Pad shorter videos by repeating last frame\n",
    "                    padding_needed = self.sequence_length - len(feature_data)\n",
    "                    if len(feature_data) > 0:\n",
    "                        last_values = feature_data.iloc[-1:].values\n",
    "                        padding = np.repeat(last_values, padding_needed, axis=0)\n",
    "                        features = np.vstack([feature_data.values, padding])\n",
    "                    else:\n",
    "                        features = np.zeros((self.sequence_length, len(self.feature_columns)), dtype=np.float32)\n",
    "                        \n",
    "                elif len(feature_data) > self.sequence_length:\n",
    "                    # For longer videos, sample random window during training\n",
    "                    if self.is_train:\n",
    "                        start_idx = np.random.randint(0, len(feature_data) - self.sequence_length + 1)\n",
    "                        features = feature_data.iloc[start_idx:start_idx + self.sequence_length].values\n",
    "                    else:\n",
    "                        # Use first window for validation\n",
    "                        features = feature_data.iloc[:self.sequence_length].values\n",
    "                else:\n",
    "                    features = feature_data.values\n",
    "                \n",
    "                # Ensure we have the right shape and type\n",
    "                features = features.astype(np.float32)\n",
    "                \n",
    "                # Handle any remaining NaN or inf values\n",
    "                features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "                \n",
    "                # Normalize features to prevent extreme values\n",
    "                features = np.clip(features, -10, 10)\n",
    "            \n",
    "            # Create SSL targets\n",
    "            ssl_targets = self._create_ssl_targets(features)\n",
    "            \n",
    "            return {\n",
    "                'features': torch.tensor(features),\n",
    "                'ssl_targets': ssl_targets,\n",
    "                'video_name': csv_path.stem\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "            # Return dummy data on error\n",
    "            dummy_features = np.zeros((self.sequence_length, self.actual_feature_dim), dtype=np.float32)\n",
    "            ssl_targets = self._create_ssl_targets(dummy_features)\n",
    "            return {\n",
    "                'features': torch.tensor(dummy_features),\n",
    "                'ssl_targets': ssl_targets,\n",
    "                'video_name': f'error_{idx}'\n",
    "            }\n",
    "    \n",
    "    def _create_ssl_targets(self, features):\n",
    "        \"\"\"Create multiple SSL objectives with fixed tensor dimensions\"\"\"\n",
    "        targets = {}\n",
    "        seq_len, feat_dim = features.shape\n",
    "        \n",
    "        # 1. Temporal Prediction: Fixed to always produce same sequence length\n",
    "        horizon = Config.PREDICTION_HORIZON\n",
    "        \n",
    "        if seq_len > horizon:\n",
    "            # Take input context from beginning, predict future frames\n",
    "            context_frames = features[:-horizon]  # Remove last 'horizon' frames\n",
    "            future_frames = features[horizon:]    # Remove first 'horizon' frames\n",
    "            \n",
    "            # Ensure both have same length (should be seq_len - horizon)\n",
    "            min_len = min(len(context_frames), len(future_frames))\n",
    "            targets['temporal_context'] = torch.tensor(context_frames[:min_len], dtype=torch.float32)\n",
    "            targets['temporal_future'] = torch.tensor(future_frames[:min_len], dtype=torch.float32)\n",
    "        else:\n",
    "            # For short sequences, predict next frame\n",
    "            targets['temporal_context'] = torch.tensor(features[:-1] if seq_len > 1 else features, dtype=torch.float32)\n",
    "            targets['temporal_future'] = torch.tensor(features[1:] if seq_len > 1 else features, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Behavioral Consistency: Use different feature subsets\n",
    "        # Split features into different behavioral modalities\n",
    "        third = feat_dim // 3\n",
    "        attention_features = features[:, :third] if third > 0 else features\n",
    "        engagement_features = features[:, third:2*third] if third > 0 else features\n",
    "        emotion_features = features[:, 2*third:] if third > 0 else features\n",
    "        \n",
    "        targets['attention_trajectory'] = torch.tensor(attention_features, dtype=torch.float32)\n",
    "        targets['engagement_trajectory'] = torch.tensor(engagement_features, dtype=torch.float32)\n",
    "        targets['emotion_trajectory'] = torch.tensor(emotion_features, dtype=torch.float32)\n",
    "        \n",
    "        # 3. Cross-modal alignment\n",
    "        targets['cross_modal_pairs'] = (\n",
    "            torch.tensor(attention_features, dtype=torch.float32),\n",
    "            torch.tensor(engagement_features, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        return targets\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):  # Increased for longer sequences\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class BehavioralTransformer(nn.Module):\n",
    "    \"\"\"Transformer designed for behavioral temporal sequences - Full scale\"\"\"\n",
    "    \n",
    "    def __init__(self, config, actual_feature_dim):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.actual_feature_dim = actual_feature_dim\n",
    "        \n",
    "        # Input projection with layer norm\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(actual_feature_dim, config.D_MODEL),\n",
    "            nn.LayerNorm(config.D_MODEL),\n",
    "            nn.Dropout(config.DROPOUT)\n",
    "        )\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(config.D_MODEL, max_len=config.SEQUENCE_LENGTH)\n",
    "        \n",
    "        # Transformer encoder - Full scale\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.D_MODEL,\n",
    "            nhead=config.N_HEADS,\n",
    "            dim_feedforward=config.D_MODEL * 4,\n",
    "            dropout=config.DROPOUT,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, config.N_ENCODER_LAYERS)\n",
    "        \n",
    "        # SSL heads with better architectures\n",
    "        self.temporal_predictor = nn.Sequential(\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.D_MODEL),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.D_MODEL // 2, actual_feature_dim)\n",
    "        )\n",
    "        \n",
    "        self.consistency_projector = nn.Sequential(\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.D_MODEL // 2),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.D_MODEL // 2, 256)  # Consistency embedding\n",
    "        )\n",
    "        \n",
    "        self.attention_flow_predictor = nn.Sequential(\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(config.D_MODEL // 2),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.D_MODEL // 2, max(actual_feature_dim // 3, 64))  # Attention flow\n",
    "        )\n",
    "        \n",
    "        # Cross-modal predictors\n",
    "        self.cross_modal_predictor = nn.Sequential(\n",
    "            nn.Linear(config.D_MODEL, config.D_MODEL // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.D_MODEL // 2, max(actual_feature_dim // 3, 64))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        # x shape: (batch, sequence, features)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to model dimension\n",
    "        x = self.input_projection(x)  # (batch, seq, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x.transpose(0, 1)  # (seq, batch, d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = x.transpose(0, 1)  # (batch, seq, d_model)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        embeddings = self.transformer(x)  # (batch, seq, d_model)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return embeddings\n",
    "        \n",
    "        # SSL predictions\n",
    "        ssl_outputs = {}\n",
    "        \n",
    "        # Temporal prediction - predict for reduced sequence length\n",
    "        prediction_length = seq_len - Config.PREDICTION_HORIZON\n",
    "        if prediction_length > 0:\n",
    "            temporal_embeddings = embeddings[:, :prediction_length]  # Match context length\n",
    "            ssl_outputs['temporal'] = self.temporal_predictor(temporal_embeddings)\n",
    "        else:\n",
    "            # Fallback for short sequences\n",
    "            ssl_outputs['temporal'] = self.temporal_predictor(embeddings[:, :-1] if seq_len > 1 else embeddings)\n",
    "        \n",
    "        # Behavioral consistency (use mean pooling)\n",
    "        ssl_outputs['consistency'] = self.consistency_projector(embeddings.mean(dim=1))\n",
    "        \n",
    "        # Attention flow\n",
    "        ssl_outputs['attention_flow'] = self.attention_flow_predictor(embeddings)\n",
    "        \n",
    "        # Cross-modal prediction\n",
    "        ssl_outputs['cross_modal'] = self.cross_modal_predictor(embeddings)\n",
    "        \n",
    "        return ssl_outputs\n",
    "\n",
    "class BehavioralSSLLoss(nn.Module):\n",
    "    \"\"\"Multi-objective SSL loss for behavioral data with fixed tensor handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.cosine_loss = nn.CosineEmbeddingLoss()\n",
    "        self.huber_loss = nn.HuberLoss(delta=1.0)\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        total_loss = 0\n",
    "        loss_components = {}\n",
    "        batch_size = None\n",
    "        device = None\n",
    "        \n",
    "        # Get batch size and device from any available tensor\n",
    "        for key, value in predictions.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch_size = value.size(0)\n",
    "                device = value.device\n",
    "                break\n",
    "        \n",
    "        if batch_size is None or device is None:\n",
    "            # Create a default tensor on CPU if no predictions available\n",
    "            device = torch.device('cpu')\n",
    "            return torch.tensor(0.0, device=device), {}\n",
    "        \n",
    "        # 1. Temporal Prediction Loss (main objective)\n",
    "        if 'temporal_future' in targets and 'temporal' in predictions:\n",
    "            future_frames = targets['temporal_future']\n",
    "            pred_frames = predictions['temporal']\n",
    "            \n",
    "            # Ensure both tensors are on the same device\n",
    "            if future_frames.device != device:\n",
    "                future_frames = future_frames.to(device)\n",
    "            \n",
    "            # Ensure batch dimension matches\n",
    "            if future_frames.size(0) == batch_size and pred_frames.size(0) == batch_size:\n",
    "                # Match sequence lengths\n",
    "                min_seq_len = min(future_frames.size(1), pred_frames.size(1))\n",
    "                min_feat_dim = min(future_frames.size(2), pred_frames.size(2))\n",
    "                \n",
    "                if min_seq_len > 0 and min_feat_dim > 0:\n",
    "                    temporal_loss = self.huber_loss(\n",
    "                        pred_frames[:, :min_seq_len, :min_feat_dim], \n",
    "                        future_frames[:, :min_seq_len, :min_feat_dim]\n",
    "                    )\n",
    "                    loss_components['temporal'] = temporal_loss\n",
    "                    total_loss += 2.0 * temporal_loss  # Higher weight for main task\n",
    "        \n",
    "        # 2. Behavioral Consistency Loss\n",
    "        if 'attention_trajectory' in targets and 'consistency' in predictions:\n",
    "            try:\n",
    "                consistency_loss = self._compute_consistency_loss(\n",
    "                    predictions['consistency'], \n",
    "                    targets['attention_trajectory'],\n",
    "                    device\n",
    "                )\n",
    "                loss_components['consistency'] = consistency_loss\n",
    "                total_loss += 0.5 * consistency_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Consistency loss error: {e}\")\n",
    "        \n",
    "        # 3. Attention Flow Smoothness Loss\n",
    "        if 'attention_flow' in predictions:\n",
    "            try:\n",
    "                flow_loss = self._compute_flow_smoothness_loss(\n",
    "                    predictions['attention_flow']\n",
    "                )\n",
    "                loss_components['flow'] = flow_loss\n",
    "                total_loss += 0.3 * flow_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Flow loss error: {e}\")\n",
    "        \n",
    "        # 4. Cross-modal alignment loss\n",
    "        if 'cross_modal_pairs' in targets and 'cross_modal' in predictions:\n",
    "            try:\n",
    "                cross_modal_loss = self._compute_cross_modal_loss(\n",
    "                    predictions['cross_modal'],\n",
    "                    targets['cross_modal_pairs'],\n",
    "                    device\n",
    "                )\n",
    "                loss_components['cross_modal'] = cross_modal_loss\n",
    "                total_loss += 0.4 * cross_modal_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Cross-modal loss error: {e}\")\n",
    "        \n",
    "        # Ensure we have at least some loss\n",
    "        if total_loss == 0:\n",
    "            total_loss = torch.tensor(0.001, device=device, requires_grad=True)\n",
    "        \n",
    "        return total_loss, loss_components\n",
    "    \n",
    "    def _compute_consistency_loss(self, embeddings, attention_trajectories, device):\n",
    "        \"\"\"Encourage similar attention patterns to have similar embeddings\"\"\"\n",
    "        batch_size = embeddings.size(0)\n",
    "        \n",
    "        if batch_size < 2:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Ensure attention_trajectories is on the correct device\n",
    "        if attention_trajectories.device != device:\n",
    "            attention_trajectories = attention_trajectories.to(device)\n",
    "        \n",
    "        # Compute attention similarity matrix\n",
    "        attention_flat = attention_trajectories.view(batch_size, -1)\n",
    "        attention_sim = F.cosine_similarity(\n",
    "            attention_flat.unsqueeze(1), \n",
    "            attention_flat.unsqueeze(0), \n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        # Compute embedding similarity matrix\n",
    "        embedding_sim = F.cosine_similarity(\n",
    "            embeddings.unsqueeze(1), \n",
    "            embeddings.unsqueeze(0), \n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        # Loss: embedding similarity should match attention similarity\n",
    "        consistency_loss = self.mse_loss(embedding_sim, attention_sim)\n",
    "        return consistency_loss\n",
    "    \n",
    "    def _compute_flow_smoothness_loss(self, attention_flow):\n",
    "        \"\"\"Encourage smooth attention transitions\"\"\"\n",
    "        if attention_flow.size(1) <= 1:\n",
    "            return torch.tensor(0.0, device=attention_flow.device)\n",
    "            \n",
    "        # Compute temporal differences\n",
    "        flow_diff = attention_flow[:, 1:] - attention_flow[:, :-1]\n",
    "        \n",
    "        # Penalize large jumps in attention flow\n",
    "        smoothness_loss = torch.mean(torch.abs(flow_diff))\n",
    "        return smoothness_loss\n",
    "    \n",
    "    def _compute_cross_modal_loss(self, predictions, target_pairs, device):\n",
    "        \"\"\"Cross-modal alignment loss\"\"\"\n",
    "        attention_targets, engagement_targets = target_pairs\n",
    "        \n",
    "        # Ensure all tensors are on the correct device\n",
    "        if attention_targets.device != device:\n",
    "            attention_targets = attention_targets.to(device)\n",
    "        if engagement_targets.device != device:\n",
    "            engagement_targets = engagement_targets.to(device)\n",
    "        \n",
    "        # Predict engagement from attention features\n",
    "        pred_engagement = predictions\n",
    "        target_engagement = engagement_targets.mean(dim=1)  # Pool over sequence: [batch, features]\n",
    "        \n",
    "        # Pool predictions over sequence to match target dimensions\n",
    "        pred_engagement_pooled = pred_engagement.mean(dim=1)  # [batch, seq, features] -> [batch, features]\n",
    "        \n",
    "        # Match dimensions\n",
    "        min_dim = min(pred_engagement_pooled.size(-1), target_engagement.size(-1))\n",
    "        \n",
    "        if min_dim > 0:\n",
    "            cross_modal_loss = self.mse_loss(\n",
    "                pred_engagement_pooled[:, :min_dim], \n",
    "                target_engagement[:, :min_dim]\n",
    "            )\n",
    "            return cross_modal_loss\n",
    "        else:\n",
    "            # Return zero loss tensor on the correct device\n",
    "            return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "\n",
    "class BehavioralSSLTrainer:\n",
    "    \"\"\"SSL Trainer for behavioral data - Full GPU utilization\"\"\"\n",
    "    \n",
    "    def __init__(self, config, actual_feature_dim):\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "        \n",
    "        # Model\n",
    "        self.model = BehavioralTransformer(config, actual_feature_dim).to(self.device)\n",
    "        \n",
    "        # Data parallel if multiple GPUs - maximize utilization\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        \n",
    "        # Loss\n",
    "        self.criterion = BehavioralSSLLoss(config)\n",
    "        \n",
    "        # Optimizer - higher learning rate for larger model\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=config.SSL_LR,\n",
    "            weight_decay=1e-4,\n",
    "            betas=(0.9, 0.95),  # Better for transformers\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        # Scheduler with warmup\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=config.SSL_LR,\n",
    "            epochs=config.SSL_EPOCHS,\n",
    "            steps_per_epoch=100,  # Approximate\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        # Gradient scaler for mixed precision\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if self.device == 'cuda' else None\n",
    "    \n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        loss_components_sum = {}\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc='Training')\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            try:\n",
    "                features = batch['features'].to(self.device, non_blocking=True)\n",
    "                ssl_targets = batch['ssl_targets']\n",
    "                \n",
    "                # Move targets to device - Fixed to handle all tensor types properly\n",
    "                for key, value in ssl_targets.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        ssl_targets[key] = value.to(self.device, non_blocking=True)\n",
    "                    elif isinstance(value, tuple):\n",
    "                        ssl_targets[key] = tuple(\n",
    "                            v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n",
    "                            for v in value\n",
    "                        )\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                if self.scaler is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        predictions = self.model(features)\n",
    "                        loss, loss_components = self.criterion(predictions, ssl_targets)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    predictions = self.model(features)\n",
    "                    loss, loss_components = self.criterion(predictions, ssl_targets)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                self.scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Accumulate loss components\n",
    "                for key, value in loss_components.items():\n",
    "                    if key not in loss_components_sum:\n",
    "                        loss_components_sum[key] = 0\n",
    "                    loss_components_sum[key] += value.item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'lr': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}',\n",
    "                    'gpu_mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return total_loss / max(num_batches, 1), loss_components_sum\n",
    "    \n",
    "    def validate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(dataloader, desc='Validation')\n",
    "            for batch in pbar:\n",
    "                try:\n",
    "                    features = batch['features'].to(self.device, non_blocking=True)\n",
    "                    ssl_targets = batch['ssl_targets']\n",
    "                    \n",
    "                    # Move targets to device - Fixed to handle all tensor types properly\n",
    "                    for key, value in ssl_targets.items():\n",
    "                        if isinstance(value, torch.Tensor):\n",
    "                            ssl_targets[key] = value.to(self.device, non_blocking=True)\n",
    "                        elif isinstance(value, tuple):\n",
    "                            ssl_targets[key] = tuple(\n",
    "                                v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n",
    "                                for v in value\n",
    "                            )\n",
    "                    \n",
    "                    if self.scaler is not None:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            predictions = self.model(features)\n",
    "                            loss, _ = self.criterion(predictions, ssl_targets)\n",
    "                    else:\n",
    "                        predictions = self.model(features)\n",
    "                        loss, _ = self.criterion(predictions, ssl_targets)\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                    pbar.set_postfix({\n",
    "                        'val_loss': f'{loss.item():.4f}',\n",
    "                        'gpu_mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        return total_loss / max(num_batches, 1)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        torch.save({\n",
    "            'model_state_dict': model_to_save.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'config': self.config,\n",
    "            'actual_feature_dim': model_to_save.actual_feature_dim\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        if hasattr(self.model, 'module'):\n",
    "            self.model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "def test_dataset_loading(data_path, max_files=5):\n",
    "    \"\"\"Test dataset loading with detailed diagnostics\"\"\"\n",
    "    print(f\"Testing dataset loading from: {data_path}\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Path does not exist: {data_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        test_dataset = BehavioralDataset(data_path, sequence_length=750, is_train=True, max_files=max_files)\n",
    "        \n",
    "        if len(test_dataset) == 0:\n",
    "            print(\"No data found in dataset\")\n",
    "            return False\n",
    "        \n",
    "        # Test loading multiple samples\n",
    "        print(f\"\\nTesting {min(3, len(test_dataset))} samples:\")\n",
    "        for i in range(min(3, len(test_dataset))):\n",
    "            sample = test_dataset[i]\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(f\"  Features shape: {sample['features'].shape}\")\n",
    "            print(f\"  Video name: {sample['video_name']}\")\n",
    "            print(f\"  SSL targets keys: {list(sample['ssl_targets'].keys())}\")\n",
    "            \n",
    "            # Check temporal target shapes\n",
    "            if 'temporal_future' in sample['ssl_targets']:\n",
    "                print(f\"  Temporal future shape: {sample['ssl_targets']['temporal_future'].shape}\")\n",
    "            if 'temporal_context' in sample['ssl_targets']:\n",
    "                print(f\"  Temporal context shape: {sample['ssl_targets']['temporal_context'].shape}\")\n",
    "            \n",
    "            # Check for actual data (not all zeros)\n",
    "            if torch.sum(torch.abs(sample['features'])) > 0:\n",
    "                print(f\"  ✓ Contains non-zero data\")\n",
    "            else:\n",
    "                print(f\"  ⚠ Warning: All zeros detected\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing dataset: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T00:07:42.915970Z",
     "iopub.status.busy": "2025-06-28T00:07:42.915372Z",
     "iopub.status.idle": "2025-06-28T00:08:18.426555Z",
     "shell.execute_reply": "2025-06-28T00:08:18.425535Z",
     "shell.execute_reply.started": "2025-06-28T00:07:42.915948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "   config = Config()\n",
    "   \n",
    "   print(\"=== Behavioral SSL Training - Full Scale ===\")\n",
    "   print(f\"Device: {config.DEVICE}\")\n",
    "   print(f\"Available GPUs: {torch.cuda.device_count()}\")     \n",
    "   print(f\"Sequence Length: {config.SEQUENCE_LENGTH} frames (30 seconds)\")\n",
    "   print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "   print(f\"Model Size: D_MODEL={config.D_MODEL}, Layers={config.N_ENCODER_LAYERS}\")\n",
    "   \n",
    "   if torch.cuda.is_available():\n",
    "       for i in range(torch.cuda.device_count()):\n",
    "           print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n",
    "           print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB\")\n",
    "   \n",
    "   # Test dataset loading first\n",
    "   print(\"\\n=== Testing Dataset Loading ===\")\n",
    "   if not test_dataset_loading(config.TRAIN_PATH, max_files=3):\n",
    "       print(\"Dataset loading test failed. Please check your data paths and format.\")\n",
    "       return\n",
    "   \n",
    "   print(\"\\n=== Creating Datasets ===\")\n",
    "   try:\n",
    "       # Create datasets with limited files for testing\n",
    "       train_dataset = BehavioralDataset(\n",
    "           config.TRAIN_PATH, \n",
    "           sequence_length=config.SEQUENCE_LENGTH, \n",
    "           is_train=True\n",
    "       )\n",
    "       \n",
    "       val_dataset = BehavioralDataset(\n",
    "           config.TEST_PATH, \n",
    "           sequence_length=config.SEQUENCE_LENGTH, \n",
    "           is_train=False\n",
    "       )\n",
    "       \n",
    "       print(f\"Train samples: {len(train_dataset)}\")\n",
    "       print(f\"Validation samples: {len(val_dataset)}\")\n",
    "       \n",
    "       # Get actual feature dimension\n",
    "       sample = train_dataset[0]\n",
    "       actual_feature_dim = sample['features'].shape[-1]\n",
    "       print(f\"Actual feature dimension: {actual_feature_dim}\")\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"Error creating datasets: {str(e)}\")\n",
    "       return\n",
    "   \n",
    "   # Create data loaders with optimized settings\n",
    "   train_loader = DataLoader(\n",
    "       train_dataset, \n",
    "       batch_size=config.BATCH_SIZE,\n",
    "       shuffle=True,\n",
    "       num_workers=2,  # Reduced for stability\n",
    "       pin_memory=True if config.DEVICE == 'cuda' else False,\n",
    "       drop_last=True\n",
    "   )\n",
    "   \n",
    "   val_loader = DataLoader(\n",
    "       val_dataset, \n",
    "       batch_size=config.BATCH_SIZE,\n",
    "       shuffle=False,\n",
    "       num_workers=2,\n",
    "       pin_memory=True if config.DEVICE == 'cuda' else False,\n",
    "       drop_last=False\n",
    "   )\n",
    "   \n",
    "   print(f\"Train batches: {len(train_loader)}\")\n",
    "   print(f\"Validation batches: {len(val_loader)}\")\n",
    "   \n",
    "   # Initialize trainer\n",
    "   print(\"\\n=== Initializing Model ===\")\n",
    "   trainer = BehavioralSSLTrainer(config, actual_feature_dim)\n",
    "   \n",
    "   # Model info\n",
    "   total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "   trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "   print(f\"Total parameters: {total_params:,}\")\n",
    "   print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "   print(f\"Model size: ~{total_params * 4 / 1e9:.2f}GB (FP32)\")\n",
    "   \n",
    "   # Training loop\n",
    "   print(\"\\n=== Starting SSL Training ===\")\n",
    "   best_val_loss = float('inf')\n",
    "   \n",
    "   for epoch in range(config.SSL_EPOCHS):\n",
    "       print(f\"\\nEpoch {epoch + 1}/{config.SSL_EPOCHS}\")\n",
    "       print(\"-\" * 50)\n",
    "       \n",
    "       # Training\n",
    "       train_loss, train_components = trainer.train_epoch(train_loader)\n",
    "       \n",
    "       # Validation\n",
    "       val_loss = trainer.validate(val_loader)\n",
    "       \n",
    "       # Print epoch results\n",
    "       print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "       print(f\"Train Loss: {train_loss:.4f}\")\n",
    "       print(f\"Val Loss: {val_loss:.4f}\")\n",
    "       \n",
    "       if train_components:\n",
    "           print(\"Training Loss Components:\")\n",
    "           for component, value in train_components.items():\n",
    "               avg_value = value / len(train_loader)\n",
    "               print(f\"  {component}: {avg_value:.4f}\")\n",
    "       \n",
    "       # Save best model\n",
    "       if val_loss < best_val_loss:\n",
    "           best_val_loss = val_loss\n",
    "           print(f\"New best validation loss: {val_loss:.4f}\")\n",
    "           trainer.save_model('best_behavioral_ssl_model.pth')\n",
    "           print(\"Model saved!\")\n",
    "       \n",
    "       # Memory cleanup\n",
    "       if torch.cuda.is_available():\n",
    "           torch.cuda.empty_cache()\n",
    "           print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB allocated, \"\n",
    "                 f\"{torch.cuda.memory_reserved()/1e9:.1f}GB reserved\")\n",
    "   \n",
    "   print(\"\\n=== Training Complete ===\")\n",
    "   print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "   print(\"Model saved as 'best_behavioral_ssl_model.pth'\")\n",
    "   \n",
    "   # Test model inference\n",
    "   print(\"\\n=== Testing Model Inference ===\")\n",
    "   try:\n",
    "       trainer.model.eval()\n",
    "       test_batch = next(iter(val_loader))\n",
    "       test_features = test_batch['features'].to(config.DEVICE)\n",
    "       \n",
    "       with torch.no_grad():\n",
    "           # Test embeddings extraction\n",
    "           embeddings = trainer.model(test_features, return_embeddings=True)\n",
    "           print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "           \n",
    "           # Test SSL predictions\n",
    "           predictions = trainer.model(test_features)\n",
    "           print(\"SSL prediction shapes:\")\n",
    "           for key, value in predictions.items():\n",
    "               print(f\"  {key}: {value.shape}\")\n",
    "       \n",
    "       print(\"✓ Model inference test successful!\")\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"Model inference test failed: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7745302,
     "sourceId": 12289458,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "vf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
